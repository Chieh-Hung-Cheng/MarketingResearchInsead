library(lavaan)
library(semPlot)
library(psych)
library(readxl)
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\interested.csv"); df
head(df)
summary(df)
hs <- df[,c(4:33)]; hs
head(hs)
hist(df$Q17_1)
# Specifying a CFA model
# To build a CFA model in lavaan, you??ll save a string with the model details.
# Each line is one latent factor, with its indicators following the =~ (read this symbol as ??is measured by??).
HS.model <- ' cirr  =~ Q17_2  + Q17_5+ Q17_6 + Q17_12 + Q17_13 + Q17_14+ Q17_17 + Q17_18 + Q17_22 + Q17_30
job =~ Q17_9 + Q17_15 + Q17_11 + Q17_21 + Q17_24 + Q17_28 + Q17_29'
fit <- cfa(HS.model, data = df,
std.lv = TRUE)
summary(fit, fit.measures = TRUE, standardized = TRUE)
fitMeasures(fit, c("cfi","srmr","gfi","rmsea"))
HS.model <- ' cirr  =~ Q17_2  + Q17_5+ Q17_6 + Q17_12 + Q17_13 + Q17_14+ Q17_17 + Q17_18 + Q17_22 + Q17_23 + Q17_30
job =~ Q17_9 + Q17_15 + Q17_11 + Q17_21 + Q17_24 + Q17_28 + Q17_29'
fit <- cfa(HS.model, data = df,
std.lv = TRUE)
summary(fit, fit.measures = TRUE, standardized = TRUE)
fitMeasures(fit, c("cfi","srmr","gfi","rmsea"))
HS.model <- ' cirr  =~ Q17_2  + Q17_5+ Q17_6 + Q17_12 + Q17_13 + Q17_14+ Q17_17 + Q17_18 + Q17_22 + Q17_30
job =~ Q17_9 + Q17_15 + Q17_11 + Q17_21 + Q17_24 + Q17_28+ Q17_27 + Q17_29'
fit <- cfa(HS.model, data = df,
std.lv = TRUE)
summary(fit, fit.measures = TRUE, standardized = TRUE)
fitMeasures(fit, c("cfi","srmr","gfi","rmsea"))
HS.model <-
' re  =~ Q17_1  + Q17_3 + Q17_7
sch =~Q17_10 + Q17_11
job = ~Q17_4+ Q17_8 + Q17_15 + Q17_22 + Q17_21 + Q17_24 + Q17_28 + Q17_29
abi =~Q17_16 + Q17_19
crr = ~Q17_12 + Q17_13 + Q17_14 + Q17_30
world = ~Q17_2 + Q17_5 + Q17_17 + Q17_18+ Q17_26'
fit <- cfa(HS.model, data = df,
std.lv = TRUE)
summary(fit, fit.measures = TRUE, standardized = TRUE)
fitMeasures(fit, c("cfi","srmr","gfi","rmsea"))
HS.model <- ' cirr  =~ Q17_2  + Q17_5+ Q17_6 + Q17_12 + Q17_13 + Q17_14+ Q17_17 + Q17_22 + Q17_30
job =~ Q17_9 + Q17_15 + Q17_11 + Q17_21 + Q17_24 + Q17_28 + Q17_29'
fit <- cfa(HS.model, data = df,
std.lv = TRUE)
summary(fit, fit.measures = TRUE, standardized = TRUE)
fitMeasures(fit, c("cfi","srmr","gfi","rmsea"))
HS.model <- ' cirr  =~ Q17_2  + Q17_5+ Q17_6 + Q17_12 + Q17_13 + Q17_14+ Q17_17 + Q17_18 + Q17_22 + Q17_30
job =~ Q17_9 + Q17_15 + Q17_11 + Q17_21 + Q17_24 + Q17_28 + Q17_29'
fit <- cfa(HS.model, data = df,
std.lv = TRUE)
summary(fit, fit.measures = TRUE, standardized = TRUE)
fitMeasures(fit, c("cfi","srmr","gfi","rmsea"))
HS.model <- ' cirr  =~ Q17_2  + Q17_5+ Q17_6 + Q17_12 + Q17_13 + Q17_14+ Q17_17 + Q17_18 + Q17_22 + Q17_30
job =~ Q17_9 + Q17_15 + Q17_11 + Q17_21 + Q17_24 + Q17_28 + Q17_29
res = ~Q17_4+Q17_8+Q17_10'
fit <- cfa(HS.model, data = df,
std.lv = TRUE)
summary(fit, fit.measures = TRUE, standardized = TRUE)
fitMeasures(fit, c("cfi","srmr","gfi","rmsea"))
#Alpha test
library(ltm)
resp = read.csv("C:\\Users\\ChiehHung\\Documents\\insead\\harvard_insead.csv"); resp
resp = read.csv("C:\\Users\\ChiehHung\\Documents\\insead\\havard_insead.csv"); resp
resp = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\havard_insead.csv"); resp
cronbach.alpha(resp[,c(7,11,18)])
# Load the data
df = read.csvx("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\potentail.csv")
# Load the data
df = read.csvx("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\potential.csv")
# Load the data
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\potential.csv")
head(df)
summary(df)
# Standardize the data
df_nmlz <- scale(df)
summary(df_nmlz)
head(df_nmlz)
head(df)
# Standardize the data
df_nmlz <- scale(df[:,3:])
# Standardize the data
df_nmlz <- scale(df[,3:])
# Standardize the data
df_nmlz <- scale(df[,3:28])
head(df_nmlz)
# Step 2.
# Compute the distance matrix
# df = the standardized data
res.dist <- dist(df_nmlz, method = "euclidean")
as.matrix(res.dist)[1:6, 1:6]
# Step 2.
# Compute the distance matrix
# df = the standardized data
res.dist <- dist(df_nmlz, method = "euclidean")
instpackages("factoextra")
install.packages("factoextra")
library("factoextra")
fviz_nbclust(df_nmlz,
FUNcluster = hcut,
method = "wss",
k.max = 10 )
# Step 2.
# Compute the distance matrix
# df = the standardized data
res.dist <- dist(df_nmlz, method = "euclidean")
as.matrix(res.dist)[1:389, 1:389]
res.hc <- hclust(d = res.dist, method = "ward.D2")
fviz_dend(res.hc, cex = 0.5)
cor(res.dist, res.coph)
res.coph <- cophenetic(res.hc)
cor(res.dist, res.coph)
fviz_nbclust(df_nmlz,
FUNcluster = hcut,
method = "wss",
k.max = 10 )
grp <- cutree(res.hc, k = 3)
grp <- cutree(res.hc, k = 3)
fviz_dend(res.hc, k = 4, rect = TRUE, cex = 0.5)
table(grp)
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\potential.csv")
head(df)
summary(df)
# Standardize
df_nmlz <- scale(df[,3:28])
head(df_nmlz)
summary(df_nmlz)
# Search # of clusters
fviz_nbclust(df_nmlz,
FUNcluster = hcut,
method = "wss",
k.max = 10 )
# Compute the distance matrix using normalized data
res.dist <- dist(df_nmlz, method = "euclidean")
as.matrix(res.dist)[1:389, 1:389]
# Clustering
res.hc <- hclust(d = res.dist, method = "ward.D2")
fviz_dend(res.hc, cex = 0.5)
res.coph <- cophenetic(res.hc)
cor(res.dist, res.coph)
# Cut and visualize
grp <- cutree(res.hc, k = 3)
fviz_dend(res.hc, k = 3, rect = TRUE, cex = 0.5)
table(grp)
rownames(df)[grp == 1]
grp == 1
df[grp == 1]
df[[grp == 1]]
grp
df[TRUE, TRUE]
df[grp==1,]
df[grp==1,]
df[grp==2,]
df[grp==3,]
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\insead_dataset_nodul.csv"); df
hs <- df[,c(4:33)]; hs
hs <- df[,c(33:53)]; hs
hs <- df[,c(33:53)]; hs
head(hs)
hs <- df[,c(33:54)]; hs
head(hs)
# EFA
# Determining # of Factors
# Parallel Analysis (PCA and FA)
fa.parallel(cor(hs), n.obs = 459, fa = "both", n.iter = 100, main = "PC analysis")
library(lavaan)
library(semPlot)
library(psych)
# EFA
# Determining # of Factors
# Parallel Analysis (PCA and FA)
fa.parallel(cor(hs), n.obs = 459, fa = "both", n.iter = 100, main = "PC analysis")
fa.parallel(cor(hs), n.obs = 459, fa = "both", n.iter = 100, main = "PC analysis")
#EFA with rotation
fa.pc <- principal(r = hs, nfactors = 5, rotate = "varimax"); fa.pc
#EFA with rotation
fa.pc <- principal(r = hs, nfactors = 5, rotate = "varimax"); fa.pc
#EFA with rotation
fa.pc <- principal(r = hs, nfactors = 3, rotate = "varimax"); fa.pc
# Load the data
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\potential_wtavg.csv")
head(df)
# Standardize
df_nmlz <- scale[df[3, 10]]# scale(df[,3:28])
# Standardize
df_nmlz <- scale[df[, 3:10]]# scale(df[,3:28])
# Standardize
df_nmlz <- scale[df[, 3:9]]# scale(df[,3:28])
head(df)
# Standardize
df_nmlz <- scale(df[, 3:10])# scale(df[,3:28])
# Standardize
df_nmlz <- scale(df[, 3:9])# scale(df[,3:28])
head(df_nmlz)
# Load the data
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\potential_wtavg.csv")
head(df)
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\potential_wtavg.csv")
head(df)
summary(df)
# Standardize
df_nmlz <- scale(df[, 3:8])# scale(df[,3:28])
head(df_nmlz)
fviz_nbclust(df_nmlz,
FUNcluster = hcut,
method = "wss",
k.max = 10 )
library("factoextra")
fviz_nbclust(df_nmlz,
FUNcluster = hcut,
method = "wss",
k.max = 10 )
res.dist <- dist(df_nmlz, method = "euclidean")
as.matrix(res.dist)[1:389, 1:389]
# Clustering
res.hc <- hclust(d = res.dist, method = "ward.D2")
fviz_dend(res.hc, cex = 0.5)
res.coph <- cophenetic(res.hc)
cor(res.dist, res.coph)
fviz_dend(res.hc, k = 3, rect = TRUE, cex = 0.5)
table(grp)
df[grp==3,]
g3 = df[grp==3,]
g1 = df[grp==1,]
g2 = df[grp==2,]
g3 = df[grp==3,]
View(g1)
View(g2)
summary(g1)
summary(g3)
summary(g1)
summary(g2)
summary(g3)
summary(g1)
summary(g2)
summary(g3)
library(lavaan)
library(semPlot)
library(psych)
library(readxl)
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\insead_dataset_nodul.csv"); df
head(df)
summary(df)
hs <- df[,c(33:54)]; hs
head(hs)
# EFA
# Determining # of Factors
# Parallel Analysis (PCA and FA)
fa.parallel(cor(hs), n.obs = 459, fa = "both", n.iter = 100, main = "PC analysis")
#EFA with rotation
fa.pc <- principal(r = hs, nfactors = 3, rotate = "varimax"); fa.pc
# Load the data
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\former_wtavg.csv")
head(df)
# Standardize
df_nmlz <- scale(df[, 4:8])# scale(df[,3:28])
head(df_nmlz)
summary(df_nmlz)
fviz_nbclust(df_nmlz,
FUNcluster = hcut,
method = "wss",
k.max = 10 )
library("factoextra")
1.
# Load the data
df = read.csv("C:\\Users\\ChiehHung\\PycharmProjects\\insead\\former_wtavg.csv")
head(df)
summary(df)
# Standardize
df_nmlz <- scale(df[, 4:8])# scale(df[,3:28])
head(df_nmlz)
summary(df_nmlz)
# Search # of clusters
fviz_nbclust(df_nmlz,
FUNcluster = hcut,
method = "wss",
k.max = 10 )
# Compute the distance matrix using normalized data
res.dist <- dist(df_nmlz, method = "euclidean")
as.matrix(res.dist)[1:70, 1:70]# [1:389, 1:389]
fviz_nbclust(df_nmlz,
FUNcluster = hcut,
method = "wss",
k.max = 10 )
# Compute the distance matrix using normalized data
res.dist <- dist(df_nmlz, method = "euclidean")
as.matrix(res.dist)[1:70, 1:70]# [1:389, 1:389]
# Clustering
res.hc <- hclust(d = res.dist, method = "ward.D2")
fviz_dend(res.hc, cex = 0.5)
res.coph <- cophenetic(res.hc)
cor(res.dist, res.coph)
# Cut and visualize
grp <- cutree(res.hc, k = 2)
fviz_dend(res.hc, k = 2, rect = TRUE, cex = 0.5)
table(grp)
g1 = df[grp==1,]
summary(g1)
g2 = df[grp==2,]
summary(g2)
fviz_dend(res.hc, k = 2, rect = TRUE, cex = 0.2)
fviz_dend(res.hc, k = 2, rect = TRUE, cex = 0.3)
g1 = df[grp==1,]
summary(g1)
g2 = df[grp==2,]
summary(g2)
